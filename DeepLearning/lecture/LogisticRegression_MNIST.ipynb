{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow load complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # 행렬 계산 라이브러리\n",
    "import matplotlib.pyplot as pit # 그래프 보여주는 라이브러리\n",
    "import scipy # 행렬 계산 라이브러리\n",
    "import tensorflow as tf   # 텐서플로우 라이브러리\n",
    "print \"tensorflow load complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data  # 텐서플로우에서 기본제공하는 MNIST(숫자필기체, 1만개) 데이터입니다\n",
    "mnist = input_data.read_data_sets('data/', one_hot=True) # OneHot Coding 이란, 정답 라벨 하나당 비트 하나를 할당하는것을 말합니다\n",
    "trainimg = mnist.train.images # 학습할 이미지\n",
    "trainlabel = mnist.train.labels # 학습할 라벨\n",
    "testimg = mnist.test.images # 테스트할 이미지\n",
    "testlabel = mnist.test.labels # 테스트할 라벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01 # W를 Optimize 1회 할때, 가중치 비율 \n",
    "training_epochs = 50  # 전체 학습 데이터에 대해서 총 반복학습 횟수\n",
    "batch_size = 100  # 10,000개 데이터 중에 한번에 몇개를 가져와서 Optimize 할것인가\n",
    "display_step = 1 # 몇번째 Epoch 마다 성능을 리포트 할것인가 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로지스틱 회귀의 그래프를 만들어 봅시다\n",
    "\n",
    "### 텐서플로우의 구조는 추상적으로 아래와 같습니다\n",
    "\n",
    "- | 먹이 : x |  -> | 레이어 | -> | 레이어 | -> ... -> | 레이어 | -> | 먹이 : y |  -> | COST | -> | 최적화 방법 |\n",
    "\n",
    "그러니까, 레이어 그래프를 정의하고, COST를 정의하고 , 최적화 방법을 정의하고,\n",
    "\n",
    "먹이 x 와 먹이 y 를 먹여(?)서, 그래프를 최적화 방법에 따라, COST를 최소화 하는 방향으로 레이어를 학습합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network constructed\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(\"float\", [None, 784])  # 먹이는 Placeholder 입니다. 먹이 x 입니다 (이미지 28x28 를 한줄로 핀 784 데이터) \n",
    "y = tf.placeholder(\"float\", [None, 10]) # 먹이 y 입니다 (라벨)\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10])) # 784개의 데이터를 10개의 데이터로 연결하는 첫번쨰 layer 입니다\n",
    "b = tf.Variable(tf.zeros([10])) # 첫번째 layer의 biases (그래프의 y 절편)\n",
    "\n",
    "actv = tf.nn.softmax(tf.matmul(x, W) + b)  #  softmax ( x * W  + b )  ->  softmax 는 결과값을 0에서 1사이로 스케일을 바꿔주는 알고리즘 입니다 (모두합하면 1)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(actv), reduction_indices=1)) # - y * log(y') = 크로스엔트로피 ( 낮을수록 비슷한 것 ) \n",
    "\n",
    "optm = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) # GradientDescentOptimizer (최적화 전략 중 하나) 는 cost를 최소화 하는방향으로\n",
    "pred = tf.equal(tf.argmax(actv, 1), tf.argmax(y, 1)) # argmax 는 가장 높은 값의 index를 가져옴, 즉 (0.0 , 0.0, 0.9 , 0.0 , 0.1 ) 이면 argmax 값은 2 인덱스\n",
    "accr = tf.reduce_mean(tf.cast(pred, \"float\")) # 맞췄는지 결과값은 batch 사이즈 만큼이므로 평균을 낸다. 이것은 정확도를 말한다\n",
    "\n",
    "init = tf.initialize_all_variables() #그래프의 모든 Variable을 초기값을 준다 (여기서는 0에 가까운 랜덤으로 초기화됨)\n",
    "print \"network constructed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/050 cost: 1.171730226 train_acc: 0.820\n",
      "Epoch: 001/050 cost: 0.655319740 train_acc: 0.860\n",
      "Epoch: 002/050 cost: 0.549059332 train_acc: 0.830\n",
      "Epoch: 003/050 cost: 0.491943460 train_acc: 0.900\n",
      "Epoch: 004/050 cost: 0.461978950 train_acc: 0.920\n",
      "Epoch: 005/050 cost: 0.437448837 train_acc: 0.870\n",
      "Epoch: 006/050 cost: 0.423181105 train_acc: 0.900\n",
      "Epoch: 007/050 cost: 0.408722305 train_acc: 0.920\n",
      "Epoch: 008/050 cost: 0.399912114 train_acc: 0.910\n",
      "Epoch: 009/050 cost: 0.393934822 train_acc: 0.870\n",
      "Epoch: 010/050 cost: 0.387194715 train_acc: 0.890\n",
      "Epoch: 011/050 cost: 0.376580075 train_acc: 0.920\n",
      "Epoch: 012/050 cost: 0.374908861 train_acc: 0.910\n",
      "Epoch: 013/050 cost: 0.368051432 train_acc: 0.890\n",
      "Epoch: 014/050 cost: 0.362041670 train_acc: 0.930\n",
      "Epoch: 015/050 cost: 0.356530867 train_acc: 0.930\n",
      "Epoch: 016/050 cost: 0.354171366 train_acc: 0.880\n",
      "Epoch: 017/050 cost: 0.351668806 train_acc: 0.920\n",
      "Epoch: 018/050 cost: 0.347968256 train_acc: 0.930\n",
      "Epoch: 019/050 cost: 0.347735364 train_acc: 0.890\n",
      "Epoch: 020/050 cost: 0.344061198 train_acc: 0.880\n",
      "Epoch: 021/050 cost: 0.336335119 train_acc: 0.920\n",
      "Epoch: 022/050 cost: 0.337768090 train_acc: 0.930\n",
      "Epoch: 023/050 cost: 0.330060094 train_acc: 0.920\n",
      "Epoch: 024/050 cost: 0.335839976 train_acc: 0.810\n",
      "Epoch: 025/050 cost: 0.330014941 train_acc: 0.910\n",
      "Epoch: 026/050 cost: 0.326910023 train_acc: 0.940\n",
      "Epoch: 027/050 cost: 0.329726906 train_acc: 0.940\n",
      "Epoch: 028/050 cost: 0.321242787 train_acc: 0.950\n",
      "Epoch: 029/050 cost: 0.322588152 train_acc: 0.910\n",
      "Epoch: 030/050 cost: 0.317801812 train_acc: 0.950\n",
      "Epoch: 031/050 cost: 0.317406359 train_acc: 0.930\n",
      "Epoch: 032/050 cost: 0.317099139 train_acc: 0.910\n",
      "Epoch: 033/050 cost: 0.320187438 train_acc: 0.850\n",
      "Epoch: 034/050 cost: 0.312084931 train_acc: 0.970\n",
      "Epoch: 035/050 cost: 0.315189460 train_acc: 0.970\n",
      "Epoch: 036/050 cost: 0.314117802 train_acc: 0.870\n",
      "Epoch: 037/050 cost: 0.313635888 train_acc: 0.890\n",
      "Epoch: 038/050 cost: 0.310758325 train_acc: 0.930\n",
      "Epoch: 039/050 cost: 0.311856226 train_acc: 0.910\n",
      "Epoch: 040/050 cost: 0.311906919 train_acc: 0.920\n",
      "Epoch: 041/050 cost: 0.311093643 train_acc: 0.930\n",
      "Epoch: 042/050 cost: 0.306464600 train_acc: 0.880\n",
      "Epoch: 043/050 cost: 0.306365708 train_acc: 0.870\n",
      "Epoch: 044/050 cost: 0.305971298 train_acc: 0.910\n",
      "Epoch: 045/050 cost: 0.309409314 train_acc: 0.950\n",
      "Epoch: 046/050 cost: 0.303887651 train_acc: 0.940\n",
      "Epoch: 047/050 cost: 0.305768883 train_acc: 0.950\n",
      "Epoch: 048/050 cost: 0.300377796 train_acc: 0.940\n",
      "Epoch: 049/050 cost: 0.305308340 train_acc: 0.970\n",
      "Optimization Finished!\n",
      "Accuracy :  0.9182\n",
      "Done. \n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init) # 그래프 구조중, 초기화 하는 코드를 실행한다. (위에 구조에 init 을 선언하였음)\n",
    "    \n",
    "    for epoch in range(training_epochs):  # training_epoch만큼 학습을 반복한다\n",
    "        avg_cost = 0.\n",
    "        num_batch = int(mnist.train.num_examples/batch_size) \n",
    "        for i in range(num_batch): # batch 사이즈 만큼 데이터를 가져와서 학습시키는것을 반복한다\n",
    "            if 0:\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size) # batch 사이즈 만큼 데이터를 가져온다\n",
    "            else:\n",
    "                randidx = np.random.randint(trainimg.shape[0], size = batch_size) # 데이터를 랜덤하게 섞어서 가져오기 위한 랜덤인덱스\n",
    "                batch_xs = trainimg[randidx, :] # batch 사이즈 만큼의 랜덤 인덱스들의 x 데이터만 뽑아서 가져온다\n",
    "                batch_ys = trainlabel[randidx, :] # batch 사이즈 만큼의 랜덤 인덱스들의 y 데이터만 뽑아서 가져온다\n",
    "                \n",
    "            sess.run(optm, feed_dict={x:batch_xs, y:batch_ys}) # optimize 를 실행한다. 이때 가져온 x 와 y를 먹이로 줘야된다\n",
    "            avg_cost += sess.run(cost, feed_dict={x:batch_xs, y:batch_ys})/num_batch #  현재 Cost를 실행한다. 이때 가져온 x 와 y를 먹이로 줘야된다 \n",
    "            \n",
    "        if epoch % display_step == 0:\n",
    "            train_acc = accr.eval({x:batch_xs, y:batch_ys}) # 현재 정확도를 구하도록 실행한다. 이때 가져온 x 와 y를 먹이로 줘야된다\n",
    "            print (\"Epoch: %03d/%03d cost: %.9f train_acc: %.3f\"\n",
    "                  % (epoch, training_epochs, avg_cost, train_acc)) # 정확도를 출력한다.\n",
    "    print \"Optimization Finished!\"\n",
    "    print \"Accuracy : \", accr.eval({x:mnist.test.images, y:mnist.test.labels}) # 학습이 끝나면 최종적으로 테스트 데이터를 먹이로 줘서 정확도를 구하도록한다\n",
    "print \"Done. \" # 끝."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
